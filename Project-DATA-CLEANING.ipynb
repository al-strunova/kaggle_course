{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7897e11",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "978c9ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fadec84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_numerical_aggs(data: pd.DataFrame,\n",
    "                          groupby_id: str,\n",
    "                          aggs: dict,\n",
    "                          prefix: Optional[str] = None,\n",
    "                          suffix: Optional[str] = None,\n",
    "                          ) -> pd.DataFrame:\n",
    "    if not prefix:\n",
    "        prefix = \"\"\n",
    "    if not suffix:\n",
    "        suffix = \"\"\n",
    "\n",
    "    data_grouped = data.groupby(groupby_id)\n",
    "    stats = data_grouped.agg(aggs)\n",
    "    stats.columns = [f\"{prefix}{feature}_{stat}{suffix}\".upper() for feature, stat in stats]\n",
    "    stats = stats.reset_index()\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8e52594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "applications_history_data = pd.read_csv('data/applications_history.csv')\n",
    "bki_data = pd.read_csv('data/bki.csv')\n",
    "client_profile_data = pd.read_csv('data/client_profile.csv')\n",
    "payments_data = pd.read_csv('data/payments.csv')\n",
    "test_data = pd.read_csv('data/test.csv')\n",
    "train_data = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "489cf0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(train_data, client_profile_data, how='left', on='APPLICATION_NUMBER', indicator='_MERGE_PROFILE')\n",
    "df_test = pd.merge(test_data, client_profile_data, how='left', on='APPLICATION_NUMBER', indicator='_MERGE_PROFILE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d53688b",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee15fa20",
   "metadata": {},
   "source": [
    "##### Drop Duplicate Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f3daac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates on the Train dataset: 0\n",
      "Number of duplicates on the Test dataset: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates\n",
    "print(\"Number of duplicates on the Train dataset:\", df_train.duplicated().sum())\n",
    "print(\"Number of duplicates on the Test dataset:\", df_test.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b25eade",
   "metadata": {},
   "source": [
    "##### Missing Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1029a3c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create new columns to flag features with a lot of missing values\n",
    "flag_missing_columns = ['OWN_CAR_AGE', 'EXTERNAL_SCORING_RATING_1', 'EXTERNAL_SCORING_RATING_2',\n",
    "                        'EXTERNAL_SCORING_RATING_3', 'AMT_REQ_CREDIT_BUREAU_MON']\n",
    "\n",
    "for column in flag_missing_columns:\n",
    "    df_train['MISSING_' + column] = ((df_train['_MERGE_PROFILE']=='both') & (df_train[column].isna())).astype('int')\n",
    "    df_test['MISSING_' + column] = ((df_test['_MERGE_PROFILE']=='both') & (df_test[column].isna())).astype('int')\n",
    "    \n",
    "# Flag DAYS_ON_LAST_JOB > 350000\n",
    "df_train['MISSING_DAYS_ON_LAST_JOB'] = (df_train.DAYS_ON_LAST_JOB > 350000).astype('int')\n",
    "df_test['MISSING_DAYS_ON_LAST_JOB'] = (df_test.DAYS_ON_LAST_JOB > 350000).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cc4502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill out missing values in numeric features as median\n",
    "df_train.loc[df_train['MISSING_OWN_CAR_AGE']==1,'MISSING_OWN_CAR_AGE'] = 0\n",
    "\n",
    "numeric_columns = df_train.drop(columns=['TARGET']).select_dtypes(include='number').columns\n",
    "for column in numeric_columns:\n",
    "    df_train[column].fillna(df_train[column].median(), inplace=True) \n",
    "    df_test[column].fillna(df_test[column].median(), inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbb5e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill out missing values in categorical features as Missing \n",
    "numeric_columns = df_train.select_dtypes(include='object').columns\n",
    "for column in numeric_columns:\n",
    "    df_train[column].fillna(\"Missing\", inplace=True) \n",
    "    df_test[column].fillna(\"Missing\", inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835ead7f",
   "metadata": {},
   "source": [
    "##### Process categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cdcce4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110093, 32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df_train.drop(columns=['TARGET'])\n",
    "target = df_train['TARGET']\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13355dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = features.select_dtypes(exclude='number').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91ee6a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.GENDER.replace('XNA', 'Missing', inplace=True)\n",
    "df_test.GENDER.replace('XNA', 'Missing', inplace=True)\n",
    "df_train.FAMILY_STATUS.replace('Unknown', 'Missing', inplace=True)\n",
    "df_test.FAMILY_STATUS.replace('Unknown', 'Missing', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b275c60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.get_dummies(df_train, cat_features)\n",
    "df_test = pd.get_dummies(df_test, cat_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fe8588",
   "metadata": {},
   "source": [
    "##### Flag outliers in numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "697dff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag outliers for 'TOTAL_SALARY', 'AMOUNT_CREDIT', 'AMOUNT_ANNUITY'\n",
    "outliers_columns = ['TOTAL_SALARY', 'AMOUNT_CREDIT', 'AMOUNT_ANNUITY']\n",
    "\n",
    "for column in outliers_columns:\n",
    "    LEFT_BOUND_train, RIGHT_BOUND_train = np.percentile(df_train[column], q=1), np.percentile(df_train[column], q=99)\n",
    "    LEFT_BOUND_test, RIGHT_BOUND_test = np.percentile(df_test[column], q=1), np.percentile(df_test[column], q=99)\n",
    "    df_train['OUTLIER_' + column] = ((df_train[column] < LEFT_BOUND_train) | (df_train[column] > RIGHT_BOUND_train)).astype('int')\n",
    "    df_test['OUTLIER_' + column] = ((df_test[column] < LEFT_BOUND_test) | (df_test[column] > RIGHT_BOUND_test)).astype('int')\n",
    "    df_train[column] = np.clip(df_train[column], LEFT_BOUND_train, RIGHT_BOUND_train)\n",
    "    df_test[column] = np.clip(df_test[column], LEFT_BOUND_test, RIGHT_BOUND_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dada189",
   "metadata": {},
   "source": [
    "##### Process numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aeb14814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make CHILDRENS as a descrete/categorical feature\n",
    "df_train['CHILDREN_0']  = (df_train.CHILDRENS == 0).astype('int')\n",
    "df_train['CHILDREN_1']  = (df_train.CHILDRENS == 1).astype('int')\n",
    "df_train['CHILDREN_2']  = (df_train.CHILDRENS == 2).astype('int')\n",
    "df_train['CHILDREN_3+']  = (df_train.CHILDRENS >= 3).astype('int')\n",
    "df_test['CHILDREN_0']  = (df_test.CHILDRENS == 0).astype('int')\n",
    "df_test['CHILDREN_1']  = (df_test.CHILDRENS == 1).astype('int')\n",
    "df_test['CHILDREN_2']  = (df_test.CHILDRENS == 2).astype('int')\n",
    "df_test['CHILDREN_3+']  = (df_test.CHILDRENS >= 3).astype('int')\n",
    "\n",
    "# Make FAMILY_SIZE as a descrete/categorical feature\n",
    "df_train['FAMILY_SIZE_0']  = (df_train.FAMILY_SIZE == 0).astype('int')\n",
    "df_train['FAMILY_SIZE_1']  = (df_train.FAMILY_SIZE == 1).astype('int')\n",
    "df_train['FAMILY_SIZE_2']  = (df_train.FAMILY_SIZE == 2).astype('int')\n",
    "df_train['FAMILY_SIZE_3+']  = (df_train.FAMILY_SIZE >= 3).astype('int')\n",
    "df_test['FAMILY_SIZE_0']  = (df_test.FAMILY_SIZE == 0).astype('int')\n",
    "df_test['FAMILY_SIZE_1']  = (df_test.FAMILY_SIZE == 1).astype('int')\n",
    "df_test['FAMILY_SIZE_2']  = (df_test.FAMILY_SIZE == 2).astype('int')\n",
    "df_test['FAMILY_SIZE_3+']  = (df_test.FAMILY_SIZE >= 3).astype('int')\n",
    "\n",
    "#df_train.drop(columns=['CHILDRENS', 'FAMILY_SIZE'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d74373",
   "metadata": {},
   "source": [
    "##### Generate new PROFILE metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e6f23ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate financial metrics\n",
    "df_train['AMOUNT_CREDIT_to_AMOUNT_ANNUITY'] = df_train['AMOUNT_CREDIT'] / df_train['AMOUNT_ANNUITY'] \n",
    "df_test['AMOUNT_CREDIT_to_AMOUNT_ANNUITY'] = df_test['AMOUNT_CREDIT'] / df_test['AMOUNT_ANNUITY'] \n",
    "\n",
    "df_train['AMOUNT_CREDIT_to_TOTAL_SALARY'] = df_train['AMOUNT_CREDIT'] / df_train['TOTAL_SALARY'] \n",
    "df_test['AMOUNT_CREDIT_to_TOTAL_SALARY'] = df_test['AMOUNT_CREDIT'] / df_test['TOTAL_SALARY'] \n",
    "\n",
    "df_train['AMOUNT_ANNUITY_to_TOTAL_SALARY'] = df_train['AMOUNT_ANNUITY'] / df_train['TOTAL_SALARY'] \n",
    "df_test['AMOUNT_ANNUITY_to_TOTAL_SALARY'] = df_test['AMOUNT_ANNUITY'] / df_test['TOTAL_SALARY'] \n",
    "\n",
    "df_train['TOTAL_SALARY_and_TOTAL_SALARY_diff'] = df_train['TOTAL_SALARY'] - df_train['AMOUNT_ANNUITY'] \n",
    "df_test['TOTAL_SALARY_and_TOTAL_SALARY_diff'] = df_test['TOTAL_SALARY'] - df_test['AMOUNT_ANNUITY'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b44f7301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate scoring metrics\n",
    "for function_name in [\"mean\", \"nanmedian\", 'min', 'max']:\n",
    "    feature_name = \"EXTERNAL_SCORING_RATINGS_{}\".format(function_name)\n",
    "    df_train[feature_name] = eval(\"np.{}\".format(function_name))(\n",
    "        df_train[[\"EXTERNAL_SCORING_RATING_1\", \"EXTERNAL_SCORING_RATING_2\", \"EXTERNAL_SCORING_RATING_3\"]], axis=1\n",
    "    )\n",
    "    df_test[feature_name] = eval(\"np.{}\".format(function_name))(\n",
    "        df_test[[\"EXTERNAL_SCORING_RATING_1\", \"EXTERNAL_SCORING_RATING_2\", \"EXTERNAL_SCORING_RATING_3\"]], axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ef03dd",
   "metadata": {},
   "source": [
    "##### Generate new applications_history_data metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c7169ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggs_refused = {\n",
    "    'PREV_APPLICATION_NUMBER': ['count'],\n",
    "    'AMT_APPLICATION': ['mean', 'min', 'max'],\n",
    "    'DAYS_DECISION': ['mean', 'min', 'max']\n",
    "}\n",
    "\n",
    "mask_refused = applications_history_data[\"NAME_CONTRACT_STATUS\"] == \"Refused\"\n",
    "stats_refused = create_numerical_aggs(\n",
    "    applications_history_data[mask_refused], groupby_id=\"APPLICATION_NUMBER\", aggs=aggs_refused, prefix=\"PREV_REFUSED_\"\n",
    ")\n",
    "\n",
    "df_train = pd.merge(df_train, stats_refused, how='left', on='APPLICATION_NUMBER')\n",
    "df_test = pd.merge(df_test, stats_refused, how='left', on='APPLICATION_NUMBER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "050398a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggs_approved = {\n",
    "    'PREV_APPLICATION_NUMBER': ['count'],\n",
    "    'AMOUNT_CREDIT': ['sum'],\n",
    "    'AMOUNT_ANNUITY': ['sum'],\n",
    "}\n",
    "\n",
    "mask_approved = applications_history_data[\"NAME_CONTRACT_STATUS\"] == \"Approved\"\n",
    "stats_approved = create_numerical_aggs(\n",
    "    applications_history_data[mask_approved], groupby_id=\"APPLICATION_NUMBER\", aggs=aggs_approved, prefix=\"PREV_APPROVED_\"\n",
    ")\n",
    "\n",
    "df_train = pd.merge(df_train, stats_approved, how='left', on='APPLICATION_NUMBER')\n",
    "df_test = pd.merge(df_test, stats_approved, how='left', on='APPLICATION_NUMBER')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67913b8",
   "metadata": {},
   "source": [
    "##### Generate new bki metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cab407f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggs = {\n",
    "    'APPLICATION_NUMBER': ['count'],\n",
    "    'CREDIT_DAY_OVERDUE': ['min', 'max'],\n",
    "    'AMT_CREDIT_MAX_OVERDUE': ['min', 'max'],\n",
    "    'CNT_CREDIT_PROLONG': ['min', 'max'],\n",
    "    'AMT_CREDIT_SUM_OVERDUE': ['min', 'max'],\n",
    "}\n",
    "\n",
    "stats = create_numerical_aggs(\n",
    "    bki_data, groupby_id=\"APPLICATION_NUMBER\", aggs=aggs, prefix=\"BKI_\"\n",
    ")\n",
    "\n",
    "df_train = pd.merge(df_train, stats, how='left', on='APPLICATION_NUMBER')\n",
    "df_test = pd.merge(df_test, stats, how='left', on='APPLICATION_NUMBER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "925bbe26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aggs_active = {\n",
    "    'APPLICATION_NUMBER': ['count'],\n",
    "    'AMT_CREDIT_SUM': ['sum'],\n",
    "    'AMT_ANNUITY': ['sum'],\n",
    "    'AMT_CREDIT_SUM_DEBT': ['sum'],\n",
    "    'AMT_CREDIT_SUM_OVERDUE': ['sum'],\n",
    "}\n",
    "\n",
    "mask_active = bki_data['CREDIT_ACTIVE'] =='Active'\n",
    "stats_active = create_numerical_aggs(\n",
    "    bki_data[mask_active], groupby_id=\"APPLICATION_NUMBER\", aggs=aggs_active, prefix=\"BKI_ACTIVE_\"\n",
    ")\n",
    "\n",
    "df_train = pd.merge(df_train, stats_active, how='left', on='APPLICATION_NUMBER')\n",
    "df_test = pd.merge(df_test, stats_active, how='left', on='APPLICATION_NUMBER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53c8b053",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggs_closed = {\n",
    "    'APPLICATION_NUMBER': ['count'],\n",
    "}\n",
    "\n",
    "mask_closed = bki_data['CREDIT_ACTIVE'] =='Closed'\n",
    "stats_closed = create_numerical_aggs(\n",
    "    bki_data[mask_closed], groupby_id=\"APPLICATION_NUMBER\", aggs=aggs_closed, prefix=\"BKI_CLOSED_\"\n",
    ")\n",
    "\n",
    "df_train = pd.merge(df_train, stats_closed, how='left', on='APPLICATION_NUMBER')\n",
    "df_test = pd.merge(df_test, stats_closed, how='left', on='APPLICATION_NUMBER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1669682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['BKI_CREDIT_SUM_and_BKI_DEBT_SUM_diff'] = df_train.BKI_ACTIVE_AMT_CREDIT_SUM_SUM - df_train.BKI_ACTIVE_AMT_CREDIT_SUM_DEBT_SUM\n",
    "df_test['BKI_CREDIT_SUM_and_BKI_DEBT_SUM_diff'] = df_test.BKI_ACTIVE_AMT_CREDIT_SUM_SUM - df_test.BKI_ACTIVE_AMT_CREDIT_SUM_DEBT_SUM\n",
    "\n",
    "df_train['NEW_and_BKI_CREDIT'] = df_train.AMOUNT_CREDIT + df_train.BKI_ACTIVE_AMT_CREDIT_SUM_SUM\n",
    "df_test['NEW_and_BKI_CREDIT'] = df_test.AMOUNT_CREDIT + df_test.BKI_ACTIVE_AMT_CREDIT_SUM_SUM\n",
    "\n",
    "df_train['NEW_and_BKI_ANNUITY'] = df_train.AMOUNT_ANNUITY + df_train.BKI_ACTIVE_AMT_ANNUITY_SUM\n",
    "df_test['NEW_and_BKI_ANNUITY'] = df_test.AMOUNT_ANNUITY + df_test.BKI_ACTIVE_AMT_ANNUITY_SUM\n",
    "\n",
    "df_train['BKI_CREDIT_to_BKI_ANNUITY'] = df_train.BKI_ACTIVE_AMT_CREDIT_SUM_SUM / df_train.BKI_ACTIVE_AMT_ANNUITY_SUM\n",
    "df_test['BKI_CREDIT_to_BKI_ANNUITY'] = df_test.BKI_ACTIVE_AMT_CREDIT_SUM_SUM / df_test.BKI_ACTIVE_AMT_ANNUITY_SUM\n",
    "\n",
    "df_train['BKI_ANNUITY_to_BKI_CREDIT'] = df_train.BKI_ACTIVE_AMT_ANNUITY_SUM / df_train.BKI_ACTIVE_AMT_CREDIT_SUM_SUM\n",
    "df_test['BKI_ANNUITY_to_BKI_CREDIT'] = df_test.BKI_ACTIVE_AMT_ANNUITY_SUM / df_test.BKI_ACTIVE_AMT_CREDIT_SUM_SUM\n",
    "\n",
    "df_train['NEW_and_BKI_CREDIT_to_ANNUITY'] = df_train.NEW_and_BKI_CREDIT / df_train.NEW_and_BKI_ANNUITY\n",
    "df_test['NEW_and_BKI_CREDIT_to_ANNUITY'] = df_test.NEW_and_BKI_CREDIT / df_test.NEW_and_BKI_ANNUITY\n",
    "\n",
    "df_train['NEW_and_BKI_ANNUITY_to_CREDIT'] = df_train.NEW_and_BKI_ANNUITY / df_train.NEW_and_BKI_CREDIT \n",
    "df_test['NEW_and_BKI_ANNUITY_to_CREDIT'] = df_test.NEW_and_BKI_ANNUITY / df_test.NEW_and_BKI_CREDIT\n",
    "\n",
    "df_train['BKI_CREDIT_to_TOTAL_SALARY'] = df_train.BKI_ACTIVE_AMT_CREDIT_SUM_SUM / df_train.TOTAL_SALARY \n",
    "df_test['BKI_CREDIT_to_TOTAL_SALARY'] = df_test.BKI_ACTIVE_AMT_CREDIT_SUM_SUM / df_test.TOTAL_SALARY\n",
    "\n",
    "df_train['NEW_and_BKI_CREDIT_to_TOTAL_SALARY'] = df_train.NEW_and_BKI_CREDIT / df_train.TOTAL_SALARY \n",
    "df_test['NEW_and_BKI_CREDIT_to_TOTAL_SALARY'] = df_test.NEW_and_BKI_CREDIT / df_test.TOTAL_SALARY\n",
    "\n",
    "df_train['BKI_ANNUITY_to_TOTAL_SALARY'] = df_train.BKI_ACTIVE_AMT_ANNUITY_SUM / df_train.TOTAL_SALARY \n",
    "df_test['BKI_ANNUITY_to_TOTAL_SALARY'] = df_test.BKI_ACTIVE_AMT_ANNUITY_SUM / df_test.TOTAL_SALARY\n",
    "\n",
    "df_train['NEW_and_BKI_ANNUITY_to_TOTAL_SALARY'] = df_train.NEW_and_BKI_ANNUITY / df_train.TOTAL_SALARY \n",
    "df_test['NEW_and_BKI_ANNUITY_to_TOTAL_SALARY'] = df_test.NEW_and_BKI_ANNUITY / df_test.TOTAL_SALARY\n",
    "\n",
    "df_train['TOTAL_SALARY_and_BKI_ANNUITY_diff'] = df_train.TOTAL_SALARY - df_train.BKI_ACTIVE_AMT_ANNUITY_SUM\n",
    "df_test['TOTAL_SALARY_and_BKI_ANNUITY_diff'] = df_test.TOTAL_SALARY - df_test.BKI_ACTIVE_AMT_ANNUITY_SUM\n",
    "\n",
    "df_train['TOTAL_SALARY_and_NEW_and_BKI_ANNUITY_diff'] = df_train.TOTAL_SALARY - df_train.NEW_and_BKI_ANNUITY\n",
    "df_test['TOTAL_SALARY_and_NEW_and_BKI_ANNUITY_diff'] = df_test.TOTAL_SALARY - df_test.NEW_and_BKI_ANNUITY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e380d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PREV_APPLICATION_NUMBER</th>\n",
       "      <th>APPLICATION_NUMBER</th>\n",
       "      <th>NUM_INSTALMENT_VERSION</th>\n",
       "      <th>NUM_INSTALMENT_NUMBER</th>\n",
       "      <th>DAYS_INSTALMENT</th>\n",
       "      <th>DAYS_ENTRY_PAYMENT</th>\n",
       "      <th>AMT_INSTALMENT</th>\n",
       "      <th>AMT_PAYMENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49011181</td>\n",
       "      <td>123664960</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5</td>\n",
       "      <td>1002.00</td>\n",
       "      <td>1015.00</td>\n",
       "      <td>12156.61</td>\n",
       "      <td>12156.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48683432</td>\n",
       "      <td>123497205</td>\n",
       "      <td>1.00</td>\n",
       "      <td>13</td>\n",
       "      <td>442.00</td>\n",
       "      <td>432.00</td>\n",
       "      <td>18392.53</td>\n",
       "      <td>10047.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48652024</td>\n",
       "      <td>123749925</td>\n",
       "      <td>1.00</td>\n",
       "      <td>10</td>\n",
       "      <td>8.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>5499.94</td>\n",
       "      <td>5499.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48398897</td>\n",
       "      <td>123550846</td>\n",
       "      <td>0.00</td>\n",
       "      <td>82</td>\n",
       "      <td>398.00</td>\n",
       "      <td>398.00</td>\n",
       "      <td>7082.15</td>\n",
       "      <td>7082.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49867197</td>\n",
       "      <td>123562174</td>\n",
       "      <td>0.00</td>\n",
       "      <td>63</td>\n",
       "      <td>1359.00</td>\n",
       "      <td>1359.00</td>\n",
       "      <td>156.74</td>\n",
       "      <td>156.74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PREV_APPLICATION_NUMBER  APPLICATION_NUMBER  NUM_INSTALMENT_VERSION  \\\n",
       "0                 49011181           123664960                    1.00   \n",
       "1                 48683432           123497205                    1.00   \n",
       "2                 48652024           123749925                    1.00   \n",
       "3                 48398897           123550846                    0.00   \n",
       "4                 49867197           123562174                    0.00   \n",
       "\n",
       "   NUM_INSTALMENT_NUMBER  DAYS_INSTALMENT  DAYS_ENTRY_PAYMENT  AMT_INSTALMENT  \\\n",
       "0                      5          1002.00             1015.00        12156.61   \n",
       "1                     13           442.00              432.00        18392.53   \n",
       "2                     10             8.00               23.00         5499.94   \n",
       "3                     82           398.00              398.00         7082.15   \n",
       "4                     63          1359.00             1359.00          156.74   \n",
       "\n",
       "   AMT_PAYMENT  \n",
       "0     12156.61  \n",
       "1     10047.65  \n",
       "2      5499.94  \n",
       "3      7082.15  \n",
       "4       156.74  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payments_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d5ebc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df_train.drop(columns=['TARGET']).columns:\n",
    "    df_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df_train[column].fillna(0, inplace=True)\n",
    "    df_test[column].fillna(0, inplace=True)\n",
    "    #df_train[column].fillna(df_train[column].median(), inplace=True) \n",
    "    #df_test[column].fillna(df_test[column].median(), inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aeee770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('data/df_train_missing_clean.csv', index=None)\n",
    "df_test.to_csv('data/df_test_missing_clean.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
